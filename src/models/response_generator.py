
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain

def generate_response(question, vector_index, api_key, model_name="gemini-1.5-flash"):
    """
    G√©n√®re une r√©ponse √† partir de la question et du contexte fourni par l'index vectoriel.
    """
    docs = vector_index.search(question)
    prompt_template =  """
        Tu es un expert universel dot√© d'une expertise approfondie dans tous les domaines. 
        Ta mission est de r√©pondre aux questions de l'utilisateur en te basant **exclusivement** sur le contexte fourni. 
        Ne fais aucune supposition en dehors du contexte et adopte une approche factuelle et analytique.

        üîπ **Contexte disponible** :
        {context}

        üîπ **Question de l'utilisateur** :
        {question}

        üîπ **Instructions pour ta r√©ponse** :
        - Fournis une r√©ponse d√©taill√©e et bien structur√©e.
        - Analyse la question en profondeur et propose des explications claires.
        - Si n√©cessaire, divise ta r√©ponse en sections pour une meilleure lisibilit√©.
        - Reste concis et pertinent : ne donne pas d‚Äôinformations inutiles.
        - Si le contexte ne permet pas de r√©pondre pr√©cis√©ment, mentionne-le plut√¥t que d'inventer une r√©ponse.

        ‚úÖ R√©ponds maintenant :
        """
    
    prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])
    model = ChatGoogleGenerativeAI(model=model_name, temperature=0.3, api_key=api_key)
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    response = chain({"input_documents": docs, "question": question}, return_only_outputs=True)
    return response['output_text']